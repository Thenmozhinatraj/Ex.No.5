

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required: 
1. ChatGPT (or any Generative AI tool supporting LLMs)

2. Text editor/Word processor for documentation

3. Spreadsheet/Tabular tool for comparison

# Explanation: 
```
Define the Two Prompt Types:

Write a basic Prompt: Clear, detailed, and structured prompts that give specific instructions or context to guide the model.
Based on that pattern type refined the prompt and submit that with AI tool.
Get the ouput and write the report.

Prepare Multiple Test Scenarios:
Select various scenarios such as:
Generating a creative story.
Answering a factual question.
Summarizing an article or concept.
Providing advice or recommendations.
Or Any other test scenario
For each scenario, create both a naïve and a basic prompt. Ensure each pair of prompts targets the same task but with different levels of structure.
Run Experiments with ChatGPT:
Input the naïve prompt for each scenario and record the generated response.
Then input the corresponding basic prompt and capture that response.
Repeat this process for all selected scenarios to gather a full set of results.
Evaluate Responses : 
	Compare how ChatGPT performs when given naïve versus basic prompts and analyze the output based on Quality,Accuracy and Depth. Also analyse does ChatGPT consistently provide better results with basic prompts? Are there scenarios where naïve prompts work equally well?
Deliverables:
A table comparing ChatGPT's responses to naïve and basic prompts across all scenarios.
Analysis of how prompt clarity impacts the quality, accuracy, and depth of ChatGPT’s outputs.
Summary of findings with insights on how to structure prompts for optimal results when using ChatGPT.
```

# OUTPUT
```
| Scenario              | Naïve Prompt Output                     | Basic Prompt Output                            | Analysis (Quality, Accuracy, Depth) |
| --------------------- | --------------------------------------- | ---------------------------------------------- | ----------------------------------- |
| Creative Story        | Short, vague story without clear flow   | Detailed, structured story with better plot    | Basic prompt gave richer output     |
| Factual Question      | Partially correct, missing details      | Accurate and well-explained answer             | Basic prompt improved accuracy      |
| Article Summarization | Very brief, missing key points          | Concise, clear summary with main ideas covered | Basic prompt provided better depth  |
| Advice/Recommendation | General suggestions, not very practical | Practical, step-by-step recommendations        | Basic prompt improved usefulness    |
Analysis

1. Naïve prompts produced vague, generic, and less structured responses.

2. Basic prompts consistently improved quality, accuracy, and depth of outputs.

3. In some creative tasks, naïve prompts still generated acceptable results, but refining the prompt added coherence and richness.

4. Clear, well-structured prompts guide the model better, reducing ambiguity and improving usability of the response.

Summary of Findings

1. Prompt clarity directly impacts output quality.

2. Basic (structured) prompts consistently outperform naïve prompts.

3. For factual, summarization, and advice tasks → structured prompts ensure accuracy and completeness.

3. For creative tasks → naïve prompts can still work but structured prompts enhance creativity and richness.
```
# Result

Thus, the experiment comparing naïve and basic prompts was executed successfully. It was observed that structured prompts improve the accuracy, quality, and depth of ChatGPT’s responses across most scenarios.
